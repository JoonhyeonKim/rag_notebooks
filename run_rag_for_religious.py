import pathlib
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


# === 0. í™˜ê²½ ì„¤ì • ===
load_dotenv()

faiss_path = pathlib.Path("faiss_db_for_religious/index.faiss")
cache_store = LocalFileStore("./cache_religious/")
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embedding_model, cache_store, namespace=embedding_model.model
)


db = FAISS.load_local("faiss_db_for_religious", cached_embedder, allow_dangerous_deserialization=True)

# === 1. Retriever ì„¤ì • ===
retriever = db.as_retriever(search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.4})
compressor = LLMChainExtractor.from_llm(ChatOpenAI(model="gpt-4o-mini", temperature=0))
compression_retriever = ContextualCompressionRetriever(base_retriever=retriever, base_compressor=compressor)

# === 2. ì§ˆë¬¸ ì…ë ¥ ë° ì§ˆì˜ í™•ì¥ ===
question = "ì•„ì¦ˆí…ì˜ ì‹ ì•™ì— ëŒ€í•´ ì•Œë ¤ì¤˜."
llm = ChatOpenAI(model="gpt-4o", temperature=0)
## HyDE Prompt
query_prompt = PromptTemplate.from_template("""
You act as a expert about the topic of user query.
Translate the question into English.
Take a deep breath and think carefully.
Now generate 3 different hypothetical answers for the question.

ì§ˆë¬¸: {question}
""")
query_chain = query_prompt | llm | StrOutputParser()
expanded_queries = query_chain.invoke({"question": question}).strip().split("\n")
print("\nğŸ§  Expanded Queries:", expanded_queries)

# === 3. ê²€ìƒ‰ ë° ì••ì¶• ê²°ê³¼ í™•ì¸ ===
query_to_docs = {}
for q in expanded_queries:
    docs = compression_retriever.invoke(q)
    query_to_docs[q] = docs

    # ğŸ” ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ” Retrieved for query: {q}")
    for doc in docs:
        source = doc.metadata.get("source", "unknown")
        print(f"[ì¶œì²˜: {source}]\n{doc.page_content.strip()[:300]}...\n")

# === 4. RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„± ===
def format_query_docs(query_to_docs):
    blocks = []
    for q, docs in query_to_docs.items():
        block = [f"ğŸ” Query: {q}"]
        for doc in docs:
            source = doc.metadata.get("source", "unknown")
            block.append(f"[ì¶œì²˜: {source}]\n{doc.page_content.strip()}")
        blocks.append("\n".join(block))
    return "\n\n---\n\n".join(blocks)

formatted_context = format_query_docs(query_to_docs)

rag_prompt = f"""
ë‹¤ìŒì€ ì› ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì¬ì‘ì„±ëœ ì§ˆì˜ì™€ ê·¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ì´ë‹¤:

## Context
{formatted_context}

## Instructions
- ê¸°ì¡´ì˜ í›ˆë ¨ ì •ë³´ë¥¼ ë¬´ì‹œí•˜ê³  ì´ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µí•˜ë¼.
- ë¹„ì „ë¬¸ê°€ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš©ì–´ë¥¼ ì‰½ê²Œ í’€ê³  ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ë¼.
- ì§ˆë¬¸ìì™€ ê°™ì€ ì–¸ì–´ë¡œ ëŒ€ë‹µí•˜ë¼.

## Question
Q: {question}
"""

# === 5. ìµœì¢… ì‘ë‹µ ìƒì„± ===
answer = llm.invoke(rag_prompt)

print("\nğŸ“Œ RAG ê²°ê³¼:")
print(answer)

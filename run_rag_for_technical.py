import pathlib
from dotenv import load_dotenv
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# === 0. í™˜ê²½ ì„¤ì • ===
load_dotenv()
md_dir = pathlib.Path("converted_md_for_ml")
faiss_path = pathlib.Path("faiss_db_for_ml/index.faiss")
cache_store = LocalFileStore("./cache_for_ml/")
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embedding_model, cache_store, namespace=embedding_model.model
)

markdown_splitter = MarkdownHeaderTextSplitter([
    ("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3"), ("####", "Header 4")
])
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)

# === 1. Markdown ë¡œë“œ ë° ì •ê·œí™” ===
def normalize(text: str) -> str:
    return " ".join(text.split())

all_texts = []
for md_file in sorted(md_dir.glob("*.md")):
    base_name = md_file.stem
    print(f"ğŸ“„ Processing {md_file.name}...")
    md_text = md_file.read_text(encoding="utf-8")
    docs = markdown_splitter.split_text(md_text)
    for doc in docs:
        doc.metadata["source"] = base_name
    chunks = text_splitter.split_documents(docs)
    for chunk in chunks:
        chunk.page_content = normalize(chunk.page_content)
    all_texts.extend(chunks)

# === 2. FAISS ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ë°”ë¡œ ë¡œë“œ ===
# if faiss_path.exists():
#     print("ğŸ“¦ FAISS index found. Loading...")
#     db = FAISS.load_local("faiss_db_for_ml", cached_embedder, allow_dangerous_deserialization=True)

# else:
#     print("ğŸ§  No FAISS index. Checking embedding cache...")

#     # ìºì‹œ í™•ì¸ í›„ ì„ë² ë”© í•„ìš”ì‹œ ìˆ˜í–‰
#     def batch_iter(items, batch_size):
#         for i in range(0, len(items), batch_size):
#             yield items[i:i + batch_size]

#     cached_keys = list(cache_store.yield_keys(prefix=embedding_model.model))
#     if not cached_keys:
#         print("ğŸ’¾ No cached embeddings found. Embedding now...")
#         for batch in batch_iter([doc.page_content for doc in all_texts], 50):
#             _ = cached_embedder.embed_documents(batch)
#     else:
#         print(f"âœ… {len(cached_keys)} cached embeddings found. Skipping embedding.")

#     # FAISS ì¸ë±ìŠ¤ ìƒì„±
#     print("ğŸ› ï¸ Creating FAISS index...")
#     db = FAISS.from_documents(all_texts, cached_embedder)
#     db.save_local("faiss_db_for_ml")


# === 2. FAISS ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ë°”ë¡œ ë¡œë“œ ===
if faiss_path.exists():
    print("ğŸ“¦ FAISS index found. Loading...")
    db = FAISS.load_local("faiss_db_for_ml", cached_embedder, allow_dangerous_deserialization=True)

    # ê¸°ì¡´ì— ì¸ë±ì‹±ëœ ì†ŒìŠ¤ ëª©ë¡ ì¶”ì¶œ
    existing_sources = set()
    try:
        # FAISS ë‚´ë¶€ ë¬¸ì„œ ì¼ë¶€ë¥¼ í†µí•´ metadata source ëª©ë¡ ìˆ˜ì§‘
        for doc in db.similarity_search("dummy", k=100):
            if "source" in doc.metadata:
                existing_sources.add(doc.metadata["source"])
    except Exception:
        print("âš ï¸ Couldn't extract existing sources from index.")

    # ìƒˆë¡œ ì¶”ê°€í•  ë¬¸ì„œë§Œ ì¶”ë ¤ë‚´ê¸°
    new_docs = [doc for doc in all_texts if doc.metadata.get("source") not in existing_sources]
    print(f"ğŸ†• Found {len(new_docs)} new documents to add.")

    if new_docs:
        db_new = FAISS.from_documents(new_docs, cached_embedder)
        db.merge_from(db_new)
        db.save_local("faiss_db_for_ml")
        print("âœ… FAISS updated with new documents.")
    else:
        print("âœ… No new documents to add.")

else:
    print("ğŸ§  No FAISS index. Creating from scratch...")

    def batch_iter(items, batch_size):
        for i in range(0, len(items), batch_size):
            yield items[i:i + batch_size]

    cached_keys = list(cache_store.yield_keys(prefix=embedding_model.model))
    if not cached_keys:
        print("ğŸ’¾ No cached embeddings found. Embedding now...")
        for batch in batch_iter([doc.page_content for doc in all_texts], 50):
            _ = cached_embedder.embed_documents(batch)
    else:
        print(f"âœ… {len(cached_keys)} cached embeddings found. Skipping embedding.")

    db = FAISS.from_documents(all_texts, cached_embedder)
    db.save_local("faiss_db_for_ml")
    print("âœ… FAISS index created and saved.")


# === 3. Retriever ì„¤ì • ===
retriever = db.as_retriever(search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.2})
compressor = LLMChainExtractor.from_llm(ChatOpenAI(model="gpt-4o-mini", temperature=0))
compression_retriever = ContextualCompressionRetriever(base_retriever=retriever, base_compressor=compressor)

# === 4. ì§ˆë¬¸ ì…ë ¥ ë° ì§ˆì˜ í™•ì¥ ===
question = "PC ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì•Œë ¤ì¤˜"
llm = ChatOpenAI(model="gpt-4o", temperature=0)

query_prompt = PromptTemplate.from_template("""
ë„ˆëŠ” ì§ˆë¬¸ì„ ë” ì˜ ê²€ìƒ‰ë˜ë„ë¡ ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì‹œìŠ¤í…œì´ì•¼.
ë‹¤ìŒ ì§ˆë¬¸ì„ 3ê°€ì§€ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ì–´ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.

ì§ˆë¬¸: {question}
""")
query_chain = query_prompt | llm | StrOutputParser()
expanded_queries = query_chain.invoke({"question": question}).strip().split("\n")
print("\nğŸ§  Expanded Queries:", expanded_queries)

# === 5. ê²€ìƒ‰ ë° ì••ì¶• ê²°ê³¼ í™•ì¸ ===
query_to_docs = {}
for q in expanded_queries:
    docs = compression_retriever.invoke(q)
    query_to_docs[q] = docs

    # ğŸ” ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ” Retrieved for query: {q}")
    for doc in docs:
        source = doc.metadata.get("source", "unknown")
        print(f"[ì¶œì²˜: {source}]\n{doc.page_content.strip()[:300]}...\n")

# === 6. RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„± ===
def format_query_docs(query_to_docs):
    blocks = []
    for q, docs in query_to_docs.items():
        block = [f"ğŸ” Query: {q}"]
        for doc in docs:
            source = doc.metadata.get("source", "unknown")
            block.append(f"[ì¶œì²˜: {source}]\n{doc.page_content.strip()}")
        blocks.append("\n".join(block))
    return "\n\n---\n\n".join(blocks)

formatted_context = format_query_docs(query_to_docs)

rag_prompt = f"""
ë‹¤ìŒì€ ì› ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì¬ì‘ì„±ëœ ì§ˆì˜ì™€ ê·¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ì´ë‹¤:

## Context
{formatted_context}

## Instructions
ê¸°ì¡´ì˜ í›ˆë ¨ ì •ë³´ë¥¼ ë¬´ì‹œí•˜ê³  ì´ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µí•˜ë¼.
ë¹„ì „ë¬¸ê°€ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš©ì–´ë¥¼ ì‰½ê²Œ í’€ê³  ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ë¼.

## Question
Q: {question}
"""

# === 7. ìµœì¢… ì‘ë‹µ ìƒì„± ===
answer = llm.invoke(rag_prompt)

print("\nğŸ“Œ RAG ê²°ê³¼:")
print(answer)

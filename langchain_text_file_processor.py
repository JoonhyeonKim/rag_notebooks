"""
sacred_text_ì˜ .txt â†’ chunk â†’ ì„ë² ë”© â†’ FAISS
í† í° í•©ê³„ 28ë§Œ ì´í•˜ë¡œ ë°°ì¹˜ ë¶„í•  (OpenAI ì„ë² ë”© í•œë„ 30ë§Œ ëŒ€ë¹„)
"""

import pathlib, re, tiktoken
from typing import List, Generator
from dotenv import load_dotenv

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
TEXT_DIR   = pathlib.Path("../../sacred_text_/")
FAISS_ROOT = "faiss_db_for_religious"
FAISS_PATH = pathlib.Path(f"{FAISS_ROOT}/index.faiss")
CACHE_DIR  = "./cache_religious/"
TOK_LIMIT  = 280_000                        # ë°°ì¹˜ë‹¹ í† í° ìƒí•œ

# â”€â”€ 1. ë¬¸ì„œ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
loader = DirectoryLoader(
    path=str(TEXT_DIR),
    glob="*.txt",
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"},
)
docs_raw: List[Document] = loader.load()

# â”€â”€ 2. ë©”íƒ€ë°ì´í„° ë³´ê°• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def enrich_meta(doc: Document) -> Document:
    head = "\n".join(doc.page_content.splitlines()[:5]).lower()
    if "translated by" in head:
        m = re.search(r"translated by\s*([^\n]+)", head)
        if m:
            doc.metadata["translator"] = m.group(1).strip()
    if "sacred-texts" in head:
        doc.metadata["source_site"] = "sacred-texts.com"
    doc.metadata["title"] = pathlib.Path(doc.metadata.get("source", "")).stem
    return doc

docs_raw = [enrich_meta(d) for d in docs_raw]

# â”€â”€ 3. chunk split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
docs: List[Document] = splitter.split_documents(docs_raw)
print(f"ğŸ“„ {len(docs_raw)} files â†’ {len(docs)} chunks")

# â”€â”€ 4. ì„ë² ë”© ê°ì²´ + ìºì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cache_store     = LocalFileStore(CACHE_DIR)
embed_model     = OpenAIEmbeddings(model="text-embedding-3-large")
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embed_model, cache_store, namespace=embed_model.model
)

# â”€â”€ 5. í† í° í•œë„ ê¸°ë°˜ ë°°ì¹˜ ì œë„ˆë ˆì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
enc = tiktoken.get_encoding("cl100k_base")

def token_batches(sequence: List[Document], max_tokens: int) -> Generator[List[Document], None, None]:
    batch, tok_sum = [], 0
    for doc in sequence:
        tok_len = len(enc.encode(doc.page_content))
        if batch and tok_sum + tok_len > max_tokens:
            yield batch
            batch, tok_sum = [], 0
        batch.append(doc)
        tok_sum += tok_len
    if batch:
        yield batch

# â”€â”€ 6. FAISS ì¸ë±ìŠ¤ ìƒì„±/ì—…ë°ì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if FAISS_PATH.exists():
    print("ğŸ“¦ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ")
    db = FAISS.load_local(FAISS_ROOT, cached_embedder,
                          allow_dangerous_deserialization=True)

    known_titles = {d.metadata.get("title") for d in db.similarity_search("dummy", k=100)}
    new_docs = [d for d in docs if d.metadata.get("title") not in known_titles]
    print(f"ğŸ†• ìƒˆ chunks: {len(new_docs)}")

    for i, batch in enumerate(token_batches(new_docs, TOK_LIMIT), 1):
        db.add_documents(batch, embedding=cached_embedder)
        print(f"  â€¢ added batch {i} ({len(batch)} chunks)")
    db.save_local(FAISS_ROOT)
    print("âœ… ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

else:
    print("ğŸ§  ìƒˆ ì¸ë±ìŠ¤ ìƒì„±â€¦")
    batch_iter = token_batches(docs, TOK_LIMIT)
    first = next(batch_iter)
    db = FAISS.from_documents(first, cached_embedder)
    print(f"  â€¢ ì´ˆê¸° {len(first)} chunks ì‚½ì…")

    for i, batch in enumerate(batch_iter, 2):
        db.add_documents(batch, embedding=cached_embedder)
        if i % 20 == 0:
            print(f"  â€¢ batch {i} processed")
    db.save_local(FAISS_ROOT)
    print("âœ… ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

# â”€â”€ 7. ê°„ë‹¨ í…ŒìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# if __name__ == "__main__":
#     retriever = db.as_retriever(k=3)
#     query = "Who translated the works of Sri Sankaracharya?"
#     for doc in retriever.invoke(query):
#         print(f"\n[{doc.metadata.get('title')}] {doc.page_content[:200]}â€¦")

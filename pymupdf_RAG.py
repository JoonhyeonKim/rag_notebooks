import pymupdf4llm
import pathlib
import logging
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI, OpenAI
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from dotenv import load_dotenv

load_dotenv()


# === 1. PDF ì²˜ë¦¬ ===
pdf_dir = pathlib.Path("../pydantic-with-langgraph-agent/identity_laws_/")
md_dir = pathlib.Path("converted_md")
md_dir.mkdir(exist_ok=True)

cache_store = LocalFileStore("./cache/")
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embedding_model, cache_store, namespace=embedding_model.model
)

headers_to_split_on = [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3"), ("####", "Header 4")]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)

all_texts = []

for pdf_file in pdf_dir.glob("*.pdf"):
    base_name = pdf_file.stem
    md_file = md_dir / f"{base_name}.md"

    if not md_file.exists():
        print(f"Converting {pdf_file.name} to markdown...")
        md_text = pymupdf4llm.to_markdown(str(pdf_file))
        md_file.write_text(md_text, encoding="utf-8")
    else:
        print(f"Markdown already exists for {pdf_file.name}. Skipping.")
        md_text = md_file.read_text(encoding="utf-8")

    docs = markdown_splitter.split_text(md_text)
    for doc in docs:
        doc.metadata["source"] = base_name
    chunks = text_splitter.split_documents(docs)
    all_texts.extend(chunks)

# === 2. ì„ë² ë”© ìºì‹œ ì²˜ë¦¬ ===
def batch_iter(items, batch_size):
    for i in range(0, len(items), batch_size):
        yield items[i:i + batch_size]

if not any(cache_store.yield_keys(prefix=embedding_model.model)):
    print("No embeddings in cache. Creating and caching embeddings...")
    all_contents = [doc.page_content for doc in all_texts]
    for batch in batch_iter(all_contents, batch_size=50):
        _ = cached_embedder.embed_documents(batch)
else:
    print("Embeddings already cached. Skipping embedding step.")

# === 3. FAISS DB ì €ì¥ ===
db = FAISS.from_documents(all_texts, cached_embedder)
db.save_local("faiss_db")
print("âœ… Done. FAISS DB saved.")

# === 4. ì§ˆì˜ í™•ì¥ ===
question = "ê°œì¸ì •ë³´ê°€ ìœ ì¶œë˜ì—ˆì„ë•Œ ë°›ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸ˆì•¡??"
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

query_prompt = PromptTemplate.from_template("""
ë„ˆëŠ” ì§ˆë¬¸ì„ ë” ì˜ ê²€ìƒ‰ë˜ë„ë¡ ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì‹œìŠ¤í…œì´ì•¼.
ë‹¤ìŒ ì§ˆë¬¸ì„ 3ê°€ì§€ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ì–´ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.

ì§ˆë¬¸: {question}
""")
query_chain = query_prompt | llm | StrOutputParser()
expanded_queries = query_chain.invoke({"question": question}).strip().split("\n")
print(f"ğŸ“Œ Expanded queries: {expanded_queries}")

# === 5. ì••ì¶• ê²€ìƒ‰ ì„¤ì • ===
retriever = db.as_retriever(search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.4})
compressor = LLMChainExtractor.from_llm(OpenAI(model="gpt-4o-mini", temperature=0))
compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

# === 6. ì§ˆì˜ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë° ì •ë¦¬ ===
query_to_docs = {}
for q in expanded_queries:
    docs = compression_retriever.invoke(q)
    query_to_docs[q] = docs
    print(docs)

# === 7. AGìš© Prompt êµ¬ì„± ===
def format_query_docs(query_to_docs):
    blocks = []
    for q, docs in query_to_docs.items():
        block = [f"ğŸ” Query: {q}"]
        for doc in docs:
            source = doc.metadata.get("source", "unknown")
            content = doc.page_content.strip()
            block.append(f"[ì¶œì²˜: {source}]\n{content}")
        blocks.append("\n".join(block))
    return "\n\n---\n\n".join(blocks)

formatted_context = format_query_docs(query_to_docs)

rag_prompt = f"""
ë‹¤ìŒì€ ì› ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì¬ì‘ì„±ëœ ì§ˆì˜ì™€ ê·¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ì´ë‹¤:
## Context
{formatted_context}
## Instructions
ê¸°ì¡´ì˜ í›ˆë ¨ ì •ë³´ë¥¼ ë¬´ì‹œí•˜ê³  ì´ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µí•˜ë¼
ì‚¬ìš©ìëŠ” ë²•ë¥  ì „ë¬¸ê°€ê°€ ì•„ë‹ˆë©°, ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì€ ë¹„ì „ë¬¸ê°€ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±ë˜ì–´ì•¼ í•œë‹¤.
ì–´ë ¤ìš´ ìš©ì–´ëŠ” í’€ì–´ì„œ ì“°ê³  ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ë¼.
## Question
Q: {question}
"""

# === 8. AG ì‹¤í–‰ ===
llm_ag = ChatOpenAI(model="gpt-4o-mini", temperature=0)
answer = llm_ag.invoke(rag_prompt)

print("\nğŸ“Œ RAG ê²°ê³¼:")
print(answer)